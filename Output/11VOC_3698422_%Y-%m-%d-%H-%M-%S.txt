save to: /users/acr23hk/paper/fsod-dc/dataset/check/voc/1726356812/base1/model_final-fsod.pth
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/setuptools/_distutils/version.py
[09/17 11:45:30 detectron2]: Rank of current process: 0. World size: 4
[09/17 11:45:30 detectron2]: Full config saved to checkpoints/voc/1726569920/fsod1/1shot/seed1/config.yaml
[09/17 11:45:31 fvcore.common.checkpoint]: [Checkpointer] Loading from ./pretrain/R-101.pkl ...
[09/17 11:45:31 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[09/17 11:45:36 detectron2]: Loss: 0.0000
[09/17 11:45:36 detectron2]: [CLS] Use dropout: p = 0.8
[09/17 11:45:36 d2.data.build]: Removed 0 images with no usable annotations. 20 images left.
[09/17 11:45:36 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=1333, sample_style='choice'), RandomFlip()]
[09/17 11:45:36 d2.data.build]: Using training sampler TrainingSampler
[09/17 11:45:36 d2.data.common]: Serializing 20 elements to byte tensors and concatenating them all ...
[09/17 11:45:36 d2.data.common]: Serialized dataset takes 0.01 MiB
[09/17 11:45:36 fvcore.common.checkpoint]: [Checkpointer] Loading from dataset/check/voc/1726356812/base1/model_final-fsod.pth ...
WARNING [09/17 11:45:36 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.fc.weight
roi_heads.box_predictor.fsup
[09/17 11:45:36 d2.engine.train_loop]: Starting training from iteration 0
[09/17 11:45:55 d2.utils.events]:  eta: 0:05:57  iter: 19  total_loss: 0.5001  loss_cls: 0.2832  loss_box_reg: 0.08153  loss_rpn_cls: 0.07835  loss_rpn_loc: 0.05362  time: 0.3667  data_time: 0.5707  lr: 0.0001  max_mem: 6703M
[09/17 11:46:02 d2.utils.events]:  eta: 0:05:50  iter: 39  total_loss: 0.3306  loss_cls: 0.1646  loss_box_reg: 0.06637  loss_rpn_cls: 0.06775  loss_rpn_loc: 0.04355  time: 0.3654  data_time: 0.0221  lr: 0.0001  max_mem: 6793M
[09/17 11:46:10 d2.utils.events]:  eta: 0:05:42  iter: 59  total_loss: 0.301  loss_cls: 0.1383  loss_box_reg: 0.06766  loss_rpn_cls: 0.05239  loss_rpn_loc: 0.04305  time: 0.3645  data_time: 0.0175  lr: 0.0001  max_mem: 6793M
[09/17 11:46:17 d2.utils.events]:  eta: 0:05:33  iter: 79  total_loss: 0.2624  loss_cls: 0.1133  loss_box_reg: 0.067  loss_rpn_cls: 0.04468  loss_rpn_loc: 0.03472  time: 0.3619  data_time: 0.0279  lr: 0.0001  max_mem: 6793M
[09/17 11:46:24 d2.utils.events]:  eta: 0:05:25  iter: 99  total_loss: 0.245  loss_cls: 0.1074  loss_box_reg: 0.06299  loss_rpn_cls: 0.046  loss_rpn_loc: 0.03189  time: 0.3622  data_time: 0.0263  lr: 0.0001  max_mem: 6793M
[09/17 11:46:31 d2.utils.events]:  eta: 0:05:18  iter: 119  total_loss: 0.2292  loss_cls: 0.09513  loss_box_reg: 0.06004  loss_rpn_cls: 0.04354  loss_rpn_loc: 0.03091  time: 0.3624  data_time: 0.0243  lr: 0.0001  max_mem: 6828M
[09/17 11:46:39 d2.utils.events]:  eta: 0:05:11  iter: 139  total_loss: 0.2311  loss_cls: 0.0918  loss_box_reg: 0.06248  loss_rpn_cls: 0.04057  loss_rpn_loc: 0.03143  time: 0.3618  data_time: 0.0319  lr: 0.0001  max_mem: 6828M
[09/17 11:46:46 d2.utils.events]:  eta: 0:05:04  iter: 159  total_loss: 0.2135  loss_cls: 0.08764  loss_box_reg: 0.05568  loss_rpn_cls: 0.03938  loss_rpn_loc: 0.03163  time: 0.3621  data_time: 0.0282  lr: 0.0001  max_mem: 6828M
[09/17 11:46:53 d2.utils.events]:  eta: 0:04:56  iter: 179  total_loss: 0.1896  loss_cls: 0.07791  loss_box_reg: 0.05509  loss_rpn_cls: 0.03151  loss_rpn_loc: 0.02434  time: 0.3621  data_time: 0.0193  lr: 0.0001  max_mem: 6828M
[09/17 11:47:00 d2.utils.events]:  eta: 0:04:49  iter: 199  total_loss: 0.1901  loss_cls: 0.07562  loss_box_reg: 0.05492  loss_rpn_cls: 0.03158  loss_rpn_loc: 0.02912  time: 0.3620  data_time: 0.0180  lr: 0.0001  max_mem: 6828M
[09/17 11:47:08 d2.utils.events]:  eta: 0:04:42  iter: 219  total_loss: 0.1872  loss_cls: 0.07449  loss_box_reg: 0.05078  loss_rpn_cls: 0.03345  loss_rpn_loc: 0.0255  time: 0.3622  data_time: 0.0276  lr: 0.0001  max_mem: 6828M
[09/17 11:47:15 d2.utils.events]:  eta: 0:04:35  iter: 239  total_loss: 0.165  loss_cls: 0.06649  loss_box_reg: 0.04673  loss_rpn_cls: 0.02726  loss_rpn_loc: 0.02422  time: 0.3621  data_time: 0.0240  lr: 0.0001  max_mem: 6828M
[09/17 11:47:22 d2.utils.events]:  eta: 0:04:28  iter: 259  total_loss: 0.1635  loss_cls: 0.06492  loss_box_reg: 0.04434  loss_rpn_cls: 0.02918  loss_rpn_loc: 0.02414  time: 0.3624  data_time: 0.0199  lr: 0.0001  max_mem: 6828M
[09/17 11:47:30 d2.utils.events]:  eta: 0:04:21  iter: 279  total_loss: 0.1571  loss_cls: 0.06276  loss_box_reg: 0.04558  loss_rpn_cls: 0.02705  loss_rpn_loc: 0.0255  time: 0.3625  data_time: 0.0211  lr: 0.0001  max_mem: 6828M
[09/17 11:47:37 d2.utils.events]:  eta: 0:04:14  iter: 299  total_loss: 0.1581  loss_cls: 0.06255  loss_box_reg: 0.04376  loss_rpn_cls: 0.02325  loss_rpn_loc: 0.02067  time: 0.3625  data_time: 0.0160  lr: 0.0001  max_mem: 6828M
[09/17 11:47:44 d2.utils.events]:  eta: 0:04:06  iter: 319  total_loss: 0.1614  loss_cls: 0.06283  loss_box_reg: 0.04443  loss_rpn_cls: 0.02889  loss_rpn_loc: 0.02748  time: 0.3625  data_time: 0.0225  lr: 0.0001  max_mem: 6828M
[09/17 11:47:51 d2.utils.events]:  eta: 0:03:59  iter: 339  total_loss: 0.1573  loss_cls: 0.06236  loss_box_reg: 0.04321  loss_rpn_cls: 0.02908  loss_rpn_loc: 0.02468  time: 0.3623  data_time: 0.0243  lr: 0.0001  max_mem: 6828M
[09/17 11:47:59 d2.utils.events]:  eta: 0:03:51  iter: 359  total_loss: 0.148  loss_cls: 0.05746  loss_box_reg: 0.04232  loss_rpn_cls: 0.02384  loss_rpn_loc: 0.022  time: 0.3624  data_time: 0.0209  lr: 0.0001  max_mem: 6828M
[09/17 11:48:06 d2.utils.events]:  eta: 0:03:44  iter: 379  total_loss: 0.1451  loss_cls: 0.05699  loss_box_reg: 0.04169  loss_rpn_cls: 0.02345  loss_rpn_loc: 0.02347  time: 0.3623  data_time: 0.0168  lr: 0.0001  max_mem: 6828M
[09/17 11:48:13 d2.utils.events]:  eta: 0:03:37  iter: 399  total_loss: 0.1371  loss_cls: 0.05287  loss_box_reg: 0.03804  loss_rpn_cls: 0.02215  loss_rpn_loc: 0.0227  time: 0.3625  data_time: 0.0230  lr: 0.0001  max_mem: 6828M
[09/17 11:48:20 d2.utils.events]:  eta: 0:03:30  iter: 419  total_loss: 0.1436  loss_cls: 0.05505  loss_box_reg: 0.03523  loss_rpn_cls: 0.0262  loss_rpn_loc: 0.0254  time: 0.3624  data_time: 0.0214  lr: 0.0001  max_mem: 6828M
[09/17 11:48:28 d2.utils.events]:  eta: 0:03:22  iter: 439  total_loss: 0.1337  loss_cls: 0.05332  loss_box_reg: 0.03775  loss_rpn_cls: 0.02451  loss_rpn_loc: 0.02724  time: 0.3623  data_time: 0.0220  lr: 0.0001  max_mem: 6828M
[09/17 11:48:35 d2.utils.events]:  eta: 0:03:15  iter: 459  total_loss: 0.1319  loss_cls: 0.05089  loss_box_reg: 0.03418  loss_rpn_cls: 0.02195  loss_rpn_loc: 0.0226  time: 0.3621  data_time: 0.0301  lr: 0.0001  max_mem: 6828M
[09/17 11:48:42 d2.utils.events]:  eta: 0:03:08  iter: 479  total_loss: 0.1389  loss_cls: 0.05278  loss_box_reg: 0.03739  loss_rpn_cls: 0.02236  loss_rpn_loc: 0.0225  time: 0.3622  data_time: 0.0252  lr: 0.0001  max_mem: 6828M
[09/17 11:48:50 d2.utils.events]:  eta: 0:03:01  iter: 499  total_loss: 0.1249  loss_cls: 0.04983  loss_box_reg: 0.03364  loss_rpn_cls: 0.02213  loss_rpn_loc: 0.02102  time: 0.3624  data_time: 0.0178  lr: 0.0001  max_mem: 6828M
[09/17 11:48:57 d2.utils.events]:  eta: 0:02:53  iter: 519  total_loss: 0.1174  loss_cls: 0.05041  loss_box_reg: 0.03541  loss_rpn_cls: 0.01963  loss_rpn_loc: 0.01888  time: 0.3624  data_time: 0.0169  lr: 0.0001  max_mem: 6828M
[09/17 11:49:04 d2.utils.events]:  eta: 0:02:46  iter: 539  total_loss: 0.1263  loss_cls: 0.04882  loss_box_reg: 0.03357  loss_rpn_cls: 0.02203  loss_rpn_loc: 0.0223  time: 0.3626  data_time: 0.0178  lr: 0.0001  max_mem: 6828M
[09/17 11:49:12 d2.utils.events]:  eta: 0:02:39  iter: 559  total_loss: 0.1233  loss_cls: 0.04643  loss_box_reg: 0.03225  loss_rpn_cls: 0.02034  loss_rpn_loc: 0.02016  time: 0.3628  data_time: 0.0219  lr: 0.0001  max_mem: 6828M
[09/17 11:49:19 d2.utils.events]:  eta: 0:02:32  iter: 579  total_loss: 0.1276  loss_cls: 0.04965  loss_box_reg: 0.0329  loss_rpn_cls: 0.01988  loss_rpn_loc: 0.01985  time: 0.3629  data_time: 0.0250  lr: 0.0001  max_mem: 6828M
[09/17 11:49:26 d2.utils.events]:  eta: 0:02:24  iter: 599  total_loss: 0.1165  loss_cls: 0.04678  loss_box_reg: 0.03159  loss_rpn_cls: 0.01931  loss_rpn_loc: 0.02045  time: 0.3628  data_time: 0.0278  lr: 0.0001  max_mem: 6828M
[09/17 11:49:33 d2.utils.events]:  eta: 0:02:17  iter: 619  total_loss: 0.1102  loss_cls: 0.04493  loss_box_reg: 0.0309  loss_rpn_cls: 0.01967  loss_rpn_loc: 0.02131  time: 0.3628  data_time: 0.0306  lr: 0.0001  max_mem: 6828M
[09/17 11:49:41 d2.utils.events]:  eta: 0:02:10  iter: 639  total_loss: 0.1175  loss_cls: 0.04781  loss_box_reg: 0.03155  loss_rpn_cls: 0.01893  loss_rpn_loc: 0.02087  time: 0.3627  data_time: 0.0213  lr: 0.0001  max_mem: 6828M
[09/17 11:49:48 d2.utils.events]:  eta: 0:02:03  iter: 659  total_loss: 0.1198  loss_cls: 0.04445  loss_box_reg: 0.0304  loss_rpn_cls: 0.02097  loss_rpn_loc: 0.02209  time: 0.3630  data_time: 0.0210  lr: 0.0001  max_mem: 6828M
[09/17 11:49:55 d2.utils.events]:  eta: 0:01:55  iter: 679  total_loss: 0.1155  loss_cls: 0.04459  loss_box_reg: 0.02929  loss_rpn_cls: 0.01949  loss_rpn_loc: 0.0185  time: 0.3627  data_time: 0.0227  lr: 0.0001  max_mem: 6828M
[09/17 11:50:02 d2.utils.events]:  eta: 0:01:48  iter: 699  total_loss: 0.1111  loss_cls: 0.04382  loss_box_reg: 0.03098  loss_rpn_cls: 0.01585  loss_rpn_loc: 0.01772  time: 0.3627  data_time: 0.0253  lr: 0.0001  max_mem: 6828M
[09/17 11:50:10 d2.utils.events]:  eta: 0:01:41  iter: 719  total_loss: 0.1156  loss_cls: 0.04434  loss_box_reg: 0.02968  loss_rpn_cls: 0.01807  loss_rpn_loc: 0.01794  time: 0.3628  data_time: 0.0237  lr: 0.0001  max_mem: 6828M
[09/17 11:50:17 d2.utils.events]:  eta: 0:01:34  iter: 739  total_loss: 0.1157  loss_cls: 0.0428  loss_box_reg: 0.03056  loss_rpn_cls: 0.02003  loss_rpn_loc: 0.01978  time: 0.3629  data_time: 0.0207  lr: 0.0001  max_mem: 6828M
[09/17 11:50:24 d2.utils.events]:  eta: 0:01:27  iter: 759  total_loss: 0.1092  loss_cls: 0.04281  loss_box_reg: 0.03091  loss_rpn_cls: 0.01673  loss_rpn_loc: 0.01751  time: 0.3629  data_time: 0.0176  lr: 0.0001  max_mem: 6828M
[09/17 11:50:32 d2.utils.events]:  eta: 0:01:19  iter: 779  total_loss: 0.1047  loss_cls: 0.04258  loss_box_reg: 0.02889  loss_rpn_cls: 0.01752  loss_rpn_loc: 0.01942  time: 0.3628  data_time: 0.0208  lr: 0.0001  max_mem: 6828M
[09/17 11:50:39 d2.utils.events]:  eta: 0:01:12  iter: 799  total_loss: 0.1029  loss_cls: 0.04192  loss_box_reg: 0.02832  loss_rpn_cls: 0.01542  loss_rpn_loc: 0.01519  time: 0.3629  data_time: 0.0205  lr: 0.0001  max_mem: 6828M
[09/17 11:50:46 d2.utils.events]:  eta: 0:01:05  iter: 819  total_loss: 0.1103  loss_cls: 0.04444  loss_box_reg: 0.02892  loss_rpn_cls: 0.0187  loss_rpn_loc: 0.01732  time: 0.3628  data_time: 0.0197  lr: 1e-05  max_mem: 6828M
[09/17 11:50:53 d2.utils.events]:  eta: 0:00:57  iter: 839  total_loss: 0.09894  loss_cls: 0.0418  loss_box_reg: 0.02749  loss_rpn_cls: 0.01576  loss_rpn_loc: 0.01453  time: 0.3629  data_time: 0.0192  lr: 1e-05  max_mem: 6828M
[09/17 11:51:01 d2.utils.events]:  eta: 0:00:50  iter: 859  total_loss: 0.1019  loss_cls: 0.04389  loss_box_reg: 0.0271  loss_rpn_cls: 0.01572  loss_rpn_loc: 0.01412  time: 0.3631  data_time: 0.0221  lr: 1e-05  max_mem: 6828M
[09/17 11:51:08 d2.utils.events]:  eta: 0:00:43  iter: 879  total_loss: 0.09455  loss_cls: 0.04315  loss_box_reg: 0.02683  loss_rpn_cls: 0.01543  loss_rpn_loc: 0.01105  time: 0.3630  data_time: 0.0226  lr: 1e-05  max_mem: 6828M
[09/17 11:51:15 d2.utils.events]:  eta: 0:00:36  iter: 899  total_loss: 0.09713  loss_cls: 0.04113  loss_box_reg: 0.02605  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.01401  time: 0.3629  data_time: 0.0219  lr: 1e-05  max_mem: 6828M
[09/17 11:51:23 d2.utils.events]:  eta: 0:00:29  iter: 919  total_loss: 0.1023  loss_cls: 0.04227  loss_box_reg: 0.02671  loss_rpn_cls: 0.01904  loss_rpn_loc: 0.01348  time: 0.3630  data_time: 0.0192  lr: 1e-05  max_mem: 6828M
[09/17 11:51:30 d2.utils.events]:  eta: 0:00:21  iter: 939  total_loss: 0.09377  loss_cls: 0.03989  loss_box_reg: 0.02698  loss_rpn_cls: 0.01407  loss_rpn_loc: 0.01412  time: 0.3631  data_time: 0.0279  lr: 1e-05  max_mem: 6828M
[09/17 11:51:37 d2.utils.events]:  eta: 0:00:14  iter: 959  total_loss: 0.09902  loss_cls: 0.0414  loss_box_reg: 0.02598  loss_rpn_cls: 0.01737  loss_rpn_loc: 0.01357  time: 0.3631  data_time: 0.0233  lr: 1e-05  max_mem: 6828M
[09/17 11:51:45 d2.utils.events]:  eta: 0:00:07  iter: 979  total_loss: 0.1022  loss_cls: 0.0414  loss_box_reg: 0.02869  loss_rpn_cls: 0.01638  loss_rpn_loc: 0.01292  time: 0.3631  data_time: 0.0235  lr: 1e-05  max_mem: 6828M
[09/17 11:51:52 fvcore.common.checkpoint]: Saving checkpoint to checkpoints/voc/1726569920/fsod1/1shot/seed1/model_final.pth
[09/17 11:51:53 d2.utils.events]:  eta: 0:00:00  iter: 999  total_loss: 0.09479  loss_cls: 0.03996  loss_box_reg: 0.02697  loss_rpn_cls: 0.01541  loss_rpn_loc: 0.01287  time: 0.3632  data_time: 0.0209  lr: 1e-05  max_mem: 6828M
[09/17 11:51:53 d2.engine.hooks]: Overall training speed: 998 iterations in 0:06:02 (0.3632 s / it)
[09/17 11:51:53 d2.engine.hooks]: Total training time: 0:06:03 (0:00:01 on hooks)
[09/17 11:52:33 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[09/17 11:52:33 d2.data.common]: Serializing 4952 elements to byte tensors and concatenating them all ...
[09/17 11:52:33 d2.data.common]: Serialized dataset takes 2.23 MiB
[09/17 11:52:33 d2.evaluation.evaluator]: Start inference on 1238 batches
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[09/17 11:52:55 d2.evaluation.evaluator]: Inference done 11/1238. Dataloading: 0.0043 s/iter. Inference: 0.0736 s/iter. Eval: 0.0003 s/iter. Total: 0.0781 s/iter. ETA=0:01:35
[09/17 11:53:00 d2.evaluation.evaluator]: Inference done 100/1238. Dataloading: 0.0025 s/iter. Inference: 0.0551 s/iter. Eval: 0.0003 s/iter. Total: 0.0582 s/iter. ETA=0:01:06
[09/17 11:53:05 d2.evaluation.evaluator]: Inference done 185/1238. Dataloading: 0.0026 s/iter. Inference: 0.0555 s/iter. Eval: 0.0003 s/iter. Total: 0.0585 s/iter. ETA=0:01:01
[09/17 11:53:10 d2.evaluation.evaluator]: Inference done 275/1238. Dataloading: 0.0024 s/iter. Inference: 0.0548 s/iter. Eval: 0.0003 s/iter. Total: 0.0577 s/iter. ETA=0:00:55
[09/17 11:53:15 d2.evaluation.evaluator]: Inference done 362/1238. Dataloading: 0.0022 s/iter. Inference: 0.0550 s/iter. Eval: 0.0003 s/iter. Total: 0.0577 s/iter. ETA=0:00:50
[09/17 11:53:20 d2.evaluation.evaluator]: Inference done 445/1238. Dataloading: 0.0022 s/iter. Inference: 0.0556 s/iter. Eval: 0.0003 s/iter. Total: 0.0582 s/iter. ETA=0:00:46
[09/17 11:53:25 d2.evaluation.evaluator]: Inference done 535/1238. Dataloading: 0.0021 s/iter. Inference: 0.0553 s/iter. Eval: 0.0003 s/iter. Total: 0.0578 s/iter. ETA=0:00:40
[09/17 11:53:30 d2.evaluation.evaluator]: Inference done 616/1238. Dataloading: 0.0022 s/iter. Inference: 0.0558 s/iter. Eval: 0.0003 s/iter. Total: 0.0584 s/iter. ETA=0:00:36
[09/17 11:53:35 d2.evaluation.evaluator]: Inference done 698/1238. Dataloading: 0.0022 s/iter. Inference: 0.0562 s/iter. Eval: 0.0003 s/iter. Total: 0.0587 s/iter. ETA=0:00:31
[09/17 11:53:40 d2.evaluation.evaluator]: Inference done 780/1238. Dataloading: 0.0022 s/iter. Inference: 0.0564 s/iter. Eval: 0.0003 s/iter. Total: 0.0590 s/iter. ETA=0:00:27
[09/17 11:53:45 d2.evaluation.evaluator]: Inference done 868/1238. Dataloading: 0.0021 s/iter. Inference: 0.0562 s/iter. Eval: 0.0003 s/iter. Total: 0.0588 s/iter. ETA=0:00:21
[09/17 11:53:50 d2.evaluation.evaluator]: Inference done 954/1238. Dataloading: 0.0022 s/iter. Inference: 0.0562 s/iter. Eval: 0.0003 s/iter. Total: 0.0588 s/iter. ETA=0:00:16
[09/17 11:53:55 d2.evaluation.evaluator]: Inference done 1044/1238. Dataloading: 0.0021 s/iter. Inference: 0.0561 s/iter. Eval: 0.0003 s/iter. Total: 0.0586 s/iter. ETA=0:00:11
[09/17 11:54:00 d2.evaluation.evaluator]: Inference done 1130/1238. Dataloading: 0.0021 s/iter. Inference: 0.0561 s/iter. Eval: 0.0003 s/iter. Total: 0.0586 s/iter. ETA=0:00:06
[09/17 11:54:05 d2.evaluation.evaluator]: Inference done 1219/1238. Dataloading: 0.0021 s/iter. Inference: 0.0559 s/iter. Eval: 0.0003 s/iter. Total: 0.0584 s/iter. ETA=0:00:01
[09/17 11:54:07 d2.evaluation.evaluator]: Total inference time: 0:01:12.564209 (0.058852 s / iter per device, on 4 devices)
[09/17 11:54:07 d2.evaluation.evaluator]: Total inference pure compute time: 0:01:08 (0.055880 s / iter per device, on 4 devices)
[09/17 11:54:10 detectron2]: Evaluating voc_2007_test_all1 using 2007 metric. Note that results do not use the official Matlab API.
[09/17 11:54:21 detectron2]: Per-class AP50:
|  aeroplane  |  bicycle  |  boat  |  bottle  |  car   |  cat   |  chair  |  diningtable  |  dog   |  horse  |  person  |  pottedplant  |  sheep  |  train  |  tvmonitor  |  bird  |  bus   |  cow   |  motorbike  |  sofa  |
|:-----------:|:---------:|:------:|:--------:|:------:|:------:|:-------:|:-------------:|:------:|:-------:|:--------:|:-------------:|:-------:|:-------:|:-----------:|:------:|:------:|:------:|:-----------:|:------:|
|   84.120    |  86.107   | 65.596 |  71.859  | 77.661 | 83.699 | 32.864  |     8.599     | 86.096 | 84.372  |  46.162  |    48.108     | 73.227  | 83.064  |   77.700    | 26.534 | 45.365 | 63.667 |   17.178    | 13.350 |
[09/17 11:54:21 detectron2]: Evaluation Result:
|   AP   |  AP50  |  AP75  |  bAP   |  bAP50  |  bAP75  |  nAP   |  nAP50  |  nAP75  |
|:------:|:------:|:------:|:------:|:-------:|:-------:|:------:|:-------:|:-------:|
| 38.714 | 58.766 | 43.593 | 44.739 | 67.282  | 50.783  | 20.638 | 33.219  | 22.021  |
[09/17 11:54:21 d2.engine.defaults]: Evaluation results for voc_2007_test_all1 in csv format:
[09/17 11:54:21 d2.evaluation.testing]: copypaste: Task: bbox
[09/17 11:54:21 d2.evaluation.testing]: copypaste: AP,AP50,AP75,bAP,bAP50,bAP75,nAP,nAP50,nAP75
[09/17 11:54:21 d2.evaluation.testing]: copypaste: 38.7136,58.7665,43.5926,44.7388,67.2823,50.7830,20.6379,33.2189,22.0213
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/setuptools/_distutils/version.py
[09/17 11:54:32 detectron2]: Rank of current process: 0. World size: 4
[09/17 11:54:32 detectron2]: Full config saved to checkpoints/voc/1726569920/fsod1/2shot/seed1/config.yaml
[09/17 11:54:33 fvcore.common.checkpoint]: [Checkpointer] Loading from ./pretrain/R-101.pkl ...
[09/17 11:54:33 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[09/17 11:54:39 detectron2]: Loss: 0.0002
[09/17 11:54:39 detectron2]: [CLS] Use dropout: p = 0.8
[09/17 11:54:39 d2.data.build]: Removed 0 images with no usable annotations. 40 images left.
[09/17 11:54:39 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=1333, sample_style='choice'), RandomFlip()]
[09/17 11:54:39 d2.data.build]: Using training sampler TrainingSampler
[09/17 11:54:40 d2.data.common]: Serializing 40 elements to byte tensors and concatenating them all ...
[09/17 11:54:40 d2.data.common]: Serialized dataset takes 0.01 MiB
[09/17 11:54:40 fvcore.common.checkpoint]: [Checkpointer] Loading from dataset/check/voc/1726356812/base1/model_final-fsod.pth ...
WARNING [09/17 11:54:40 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.fc.weight
roi_heads.box_predictor.fsup
[09/17 11:54:40 d2.engine.train_loop]: Starting training from iteration 0
ERROR [09/17 11:54:51 d2.engine.train_loop]: Exception during training:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!
[09/17 11:54:51 d2.engine.hooks]: Total training time: 0:00:10 (0:00:00 on hooks)
[09/17 11:54:51 d2.utils.events]:  iter: 0    lr: N/A  max_mem: 4142M
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-3:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 7817) is killed by signal: Terminated. 
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-2:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 7836) is killed by signal: Terminated. 
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-4:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 7857) is killed by signal: Terminated. 
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 7818) is killed by signal: Terminated. 
Traceback (most recent call last):
  File "main.py", line 38, in <module>
    launch(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 67, in launch
    mp.spawn(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/setuptools/_distutils/version.py
[09/17 11:55:01 detectron2]: Rank of current process: 0. World size: 4
[09/17 11:55:01 detectron2]: Full config saved to checkpoints/voc/1726569920/fsod1/3shot/seed1/config.yaml
[09/17 11:55:02 fvcore.common.checkpoint]: [Checkpointer] Loading from ./pretrain/R-101.pkl ...
[09/17 11:55:02 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[09/17 11:55:09 detectron2]: Loss: 0.0003
[09/17 11:55:09 detectron2]: [CLS] Use dropout: p = 0.8
[09/17 11:55:10 d2.data.build]: Removed 0 images with no usable annotations. 60 images left.
[09/17 11:55:10 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=1333, sample_style='choice'), RandomFlip()]
[09/17 11:55:10 d2.data.build]: Using training sampler TrainingSampler
[09/17 11:55:10 d2.data.common]: Serializing 60 elements to byte tensors and concatenating them all ...
[09/17 11:55:10 d2.data.common]: Serialized dataset takes 0.02 MiB
[09/17 11:55:10 fvcore.common.checkpoint]: [Checkpointer] Loading from dataset/check/voc/1726356812/base1/model_final-fsod.pth ...
WARNING [09/17 11:55:10 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.fc.weight
roi_heads.box_predictor.fsup
[09/17 11:55:10 d2.engine.train_loop]: Starting training from iteration 0
ERROR [09/17 11:55:20 d2.engine.train_loop]: Exception during training:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!
[09/17 11:55:20 d2.engine.hooks]: Total training time: 0:00:10 (0:00:00 on hooks)
[09/17 11:55:20 d2.utils.events]:  iter: 0    lr: N/A  max_mem: 4372M
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-2:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 9809) is killed by signal: Terminated. 
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-4:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 9820) is killed by signal: Terminated. 
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 9815) is killed by signal: Terminated. 
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-3:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 9827) is killed by signal: Terminated. 
Traceback (most recent call last):
  File "main.py", line 38, in <module>
    launch(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 67, in launch
    mp.spawn(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/setuptools/_distutils/version.py
[09/17 11:55:32 detectron2]: Rank of current process: 0. World size: 4
[09/17 11:55:32 detectron2]: Full config saved to checkpoints/voc/1726569920/fsod1/5shot/seed1/config.yaml
[09/17 11:55:33 fvcore.common.checkpoint]: [Checkpointer] Loading from ./pretrain/R-101.pkl ...
[09/17 11:55:33 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[09/17 11:55:42 detectron2]: Loss: 0.0005
[09/17 11:55:42 detectron2]: [CLS] Use dropout: p = 0.8
[09/17 11:55:43 d2.data.build]: Removed 0 images with no usable annotations. 100 images left.
[09/17 11:55:43 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=1333, sample_style='choice'), RandomFlip()]
[09/17 11:55:43 d2.data.build]: Using training sampler TrainingSampler
[09/17 11:55:43 d2.data.common]: Serializing 100 elements to byte tensors and concatenating them all ...
[09/17 11:55:43 d2.data.common]: Serialized dataset takes 0.03 MiB
[09/17 11:55:43 fvcore.common.checkpoint]: [Checkpointer] Loading from dataset/check/voc/1726356812/base1/model_final-fsod.pth ...
WARNING [09/17 11:55:43 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.fc.weight
roi_heads.box_predictor.fsup
[09/17 11:55:43 d2.engine.train_loop]: Starting training from iteration 0
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-3:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 11783) is killed by signal: Terminated. 
ERROR [09/17 11:55:54 d2.engine.train_loop]: Exception during training:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!
[09/17 11:55:54 d2.engine.hooks]: Total training time: 0:00:11 (0:00:00 on hooks)
[09/17 11:55:54 d2.utils.events]:  iter: 0    lr: N/A  max_mem: 4139M
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 11849) is killed by signal: Terminated. 
Traceback (most recent call last):
  File "main.py", line 38, in <module>
    launch(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 67, in launch
    mp.spawn(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/setuptools/_distutils/version.py
[09/17 11:56:04 detectron2]: Rank of current process: 0. World size: 4
[09/17 11:56:04 detectron2]: Full config saved to checkpoints/voc/1726569920/fsod1/10shot/seed1/config.yaml
[09/17 11:56:05 fvcore.common.checkpoint]: [Checkpointer] Loading from ./pretrain/R-101.pkl ...
[09/17 11:56:05 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[09/17 11:56:20 detectron2]: Loss: 0.0010
[09/17 11:56:20 detectron2]: [CLS] Use dropout: p = 0.8
[09/17 11:56:20 d2.data.build]: Removed 0 images with no usable annotations. 200 images left.
[09/17 11:56:20 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=1333, sample_style='choice'), RandomFlip()]
[09/17 11:56:20 d2.data.build]: Using training sampler TrainingSampler
[09/17 11:56:21 d2.data.common]: Serializing 200 elements to byte tensors and concatenating them all ...
[09/17 11:56:21 d2.data.common]: Serialized dataset takes 0.05 MiB
[09/17 11:56:21 fvcore.common.checkpoint]: [Checkpointer] Loading from dataset/check/voc/1726356812/base1/model_final-fsod.pth ...
WARNING [09/17 11:56:21 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.fc.weight
roi_heads.box_predictor.fsup
[09/17 11:56:21 d2.engine.train_loop]: Starting training from iteration 0
ERROR [09/17 11:56:34 d2.engine.train_loop]: Exception during training:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!
[09/17 11:56:34 d2.engine.hooks]: Total training time: 0:00:13 (0:00:00 on hooks)
[09/17 11:56:34 d2.utils.events]:  iter: 0    lr: N/A  max_mem: 4138M
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "main.py", line 38, in <module>
    launch(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 67, in launch
    mp.spawn(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 68, in update_centroids
    confidence = self.calculate_confidence(features, proposals)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in calculate_confidence
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 101, in <listcomp>
    contour_scores = torch.tensor([x.contour_score for x in proposals]).cuda()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/structures/instances.py", line 65, in __getattr__
    raise AttributeError("Cannot find field '{}' in the given Instances!".format(name))
AttributeError: Cannot find field 'contour_score' in the given Instances!

┌───────┬──────────────────────────────┬──────────────────────────────┬──────────────────────────────┐
│ Split │              1               │              2               │              3               │
├───────┼──────────────────────────────┼──────────────────────────────┼──────────────────────────────┤
│   k   │  1     2     3     5     10  │  1     2     3     5     10  │  1     2     3     5     10  │
├───────┼──────────────────────────────┼──────────────────────────────┼──────────────────────────────┤
│  AP50 │ 58.8   -     -     -     -   │  -     -     -     -     -   │  -     -     -     -     -   │
│ bAP50 │ 67.3   -     -     -     -   │  -     -     -     -     -   │  -     -     -     -     -   │
│ nAP50 │ 33.2   -     -     -     -   │  -     -     -     -     -   │  -     -     -     -     -   │
└───────┴──────────────────────────────┴──────────────────────────────┴──────────────────────────────┘
