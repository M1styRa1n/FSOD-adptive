save to: /users/acr23hk/paper/fsod-dc/dataset/check/voc/1726356812/base1/model_final-fsod.pth
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/setuptools/_distutils/version.py
[09/17 12:34:49 detectron2]: Rank of current process: 0. World size: 4
[09/17 12:34:49 detectron2]: Full config saved to checkpoints/voc/1726572880/fsod1/1shot/seed1/config.yaml
[09/17 12:34:50 fvcore.common.checkpoint]: [Checkpointer] Loading from ./pretrain/R-101.pkl ...
[09/17 12:34:50 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[09/17 12:34:55 detectron2]: Loss: 0.0000
[09/17 12:34:55 detectron2]: [CLS] Use dropout: p = 0.8
[09/17 12:34:55 d2.data.build]: Removed 0 images with no usable annotations. 20 images left.
[09/17 12:34:55 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=1333, sample_style='choice'), RandomFlip()]
[09/17 12:34:55 d2.data.build]: Using training sampler TrainingSampler
[09/17 12:34:55 d2.data.common]: Serializing 20 elements to byte tensors and concatenating them all ...
[09/17 12:34:55 d2.data.common]: Serialized dataset takes 0.01 MiB
[09/17 12:34:56 fvcore.common.checkpoint]: [Checkpointer] Loading from dataset/check/voc/1726356812/base1/model_final-fsod.pth ...
WARNING [09/17 12:34:56 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.fc.weight
roi_heads.box_predictor.fsup
[09/17 12:34:56 d2.engine.train_loop]: Starting training from iteration 0
ERROR [09/17 12:35:07 d2.engine.train_loop]: Exception during training:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 94, in update_centroids
    self.centroids.set_((1 - alpha) * self.centroids + alpha * centroids)
RuntimeError: The size of tensor a (1024) must match the size of tensor b (24) at non-singleton dimension 0
[09/17 12:35:07 d2.engine.hooks]: Total training time: 0:00:11 (0:00:00 on hooks)
[09/17 12:35:07 d2.utils.events]:  iter: 0    lr: N/A  max_mem: 3854M
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "main.py", line 38, in <module>
    launch(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 67, in launch
    mp.spawn(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 94, in update_centroids
    self.centroids.set_((1 - alpha) * self.centroids + alpha * centroids)
RuntimeError: The size of tensor a (1024) must match the size of tensor b (24) at non-singleton dimension 0

/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/setuptools/_distutils/version.py
[09/17 12:35:17 detectron2]: Rank of current process: 0. World size: 4
[09/17 12:35:17 detectron2]: Full config saved to checkpoints/voc/1726572880/fsod1/2shot/seed1/config.yaml
[09/17 12:35:18 fvcore.common.checkpoint]: [Checkpointer] Loading from ./pretrain/R-101.pkl ...
[09/17 12:35:18 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[09/17 12:35:24 detectron2]: Loss: 0.0002
[09/17 12:35:24 detectron2]: [CLS] Use dropout: p = 0.8
[09/17 12:35:24 d2.data.build]: Removed 0 images with no usable annotations. 40 images left.
[09/17 12:35:24 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=1333, sample_style='choice'), RandomFlip()]
[09/17 12:35:24 d2.data.build]: Using training sampler TrainingSampler
[09/17 12:35:24 d2.data.common]: Serializing 40 elements to byte tensors and concatenating them all ...
[09/17 12:35:24 d2.data.common]: Serialized dataset takes 0.01 MiB
[09/17 12:35:25 fvcore.common.checkpoint]: [Checkpointer] Loading from dataset/check/voc/1726356812/base1/model_final-fsod.pth ...
WARNING [09/17 12:35:25 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.fc.weight
roi_heads.box_predictor.fsup
[09/17 12:35:25 d2.engine.train_loop]: Starting training from iteration 0
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-2:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 94, in update_centroids
    self.centroids.set_((1 - alpha) * self.centroids + alpha * centroids)
RuntimeError: The size of tensor a (1024) must match the size of tensor b (24) at non-singleton dimension 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 36109) is killed by signal: Terminated. 
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-3:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 94, in update_centroids
    self.centroids.set_((1 - alpha) * self.centroids + alpha * centroids)
RuntimeError: The size of tensor a (1024) must match the size of tensor b (24) at non-singleton dimension 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 36101) is killed by signal: Terminated. 
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Process SpawnProcess-4:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 94, in update_centroids
    self.centroids.set_((1 - alpha) * self.centroids + alpha * centroids)
RuntimeError: The size of tensor a (1024) must match the size of tensor b (24) at non-singleton dimension 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 66, in _wrap
    sys.exit(1)
SystemExit: 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 318, in _bootstrap
    util._exit_function()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/util.py", line 357, in _exit_function
    p.join()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 47, in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 36135) is killed by signal: Terminated. 
ERROR [09/17 12:35:36 d2.engine.train_loop]: Exception during training:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 94, in update_centroids
    self.centroids.set_((1 - alpha) * self.centroids + alpha * centroids)
RuntimeError: The size of tensor a (1024) must match the size of tensor b (24) at non-singleton dimension 0
[09/17 12:35:36 d2.engine.hooks]: Total training time: 0:00:10 (0:00:00 on hooks)
[09/17 12:35:36 d2.utils.events]:  iter: 0    lr: N/A  max_mem: 4142M
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "main.py", line 38, in <module>
    launch(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 67, in launch
    mp.spawn(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 94, in update_centroids
    self.centroids.set_((1 - alpha) * self.centroids + alpha * centroids)
RuntimeError: The size of tensor a (1024) must match the size of tensor b (24) at non-singleton dimension 0

/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/setuptools/_distutils/version.py
[09/17 12:35:45 detectron2]: Rank of current process: 0. World size: 4
[09/17 12:35:45 detectron2]: Full config saved to checkpoints/voc/1726572880/fsod1/3shot/seed1/config.yaml
[09/17 12:35:47 fvcore.common.checkpoint]: [Checkpointer] Loading from ./pretrain/R-101.pkl ...
[09/17 12:35:47 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[09/17 12:35:54 detectron2]: Loss: 0.0003
[09/17 12:35:54 detectron2]: [CLS] Use dropout: p = 0.8
[09/17 12:35:54 d2.data.build]: Removed 0 images with no usable annotations. 60 images left.
[09/17 12:35:54 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=1333, sample_style='choice'), RandomFlip()]
[09/17 12:35:54 d2.data.build]: Using training sampler TrainingSampler
[09/17 12:35:54 d2.data.common]: Serializing 60 elements to byte tensors and concatenating them all ...
[09/17 12:35:54 d2.data.common]: Serialized dataset takes 0.02 MiB
[09/17 12:35:54 fvcore.common.checkpoint]: [Checkpointer] Loading from dataset/check/voc/1726356812/base1/model_final-fsod.pth ...
WARNING [09/17 12:35:54 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.fc.weight
roi_heads.box_predictor.fsup
[09/17 12:35:54 d2.engine.train_loop]: Starting training from iteration 0
ERROR [09/17 12:36:07 d2.engine.train_loop]: Exception during training:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 94, in update_centroids
    self.centroids.set_((1 - alpha) * self.centroids + alpha * centroids)
RuntimeError: The size of tensor a (1024) must match the size of tensor b (24) at non-singleton dimension 0
[09/17 12:36:07 d2.engine.hooks]: Total training time: 0:00:12 (0:00:00 on hooks)
[09/17 12:36:07 d2.utils.events]:  iter: 0    lr: N/A  max_mem: 4372M
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "main.py", line 38, in <module>
    launch(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 67, in launch
    mp.spawn(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 273, in run_step
    loss_dict = self.model(data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 54, in forward
    self.refine.update_centroids(features["res4"], proposals)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 94, in update_centroids
    self.centroids.set_((1 - alpha) * self.centroids + alpha * centroids)
RuntimeError: The size of tensor a (1024) must match the size of tensor b (24) at non-singleton dimension 0

/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/setuptools/_distutils/version.py
[09/17 12:36:16 detectron2]: Rank of current process: 0. World size: 4
[09/17 12:36:16 detectron2]: Full config saved to checkpoints/voc/1726572880/fsod1/5shot/seed1/config.yaml
[09/17 12:36:18 fvcore.common.checkpoint]: [Checkpointer] Loading from ./pretrain/R-101.pkl ...
[09/17 12:36:18 fvcore.common.checkpoint]: Reading a file from 'torchvision'
[09/17 12:36:27 detectron2]: Loss: 0.0005
[09/17 12:36:27 detectron2]: [CLS] Use dropout: p = 0.8
[09/17 12:36:27 d2.data.build]: Removed 0 images with no usable annotations. 100 images left.
[09/17 12:36:27 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=..., max_size=1333, sample_style='choice'), RandomFlip()]
[09/17 12:36:27 d2.data.build]: Using training sampler TrainingSampler
[09/17 12:36:27 d2.data.common]: Serializing 100 elements to byte tensors and concatenating them all ...
[09/17 12:36:27 d2.data.common]: Serialized dataset takes 0.03 MiB
[09/17 12:36:27 fvcore.common.checkpoint]: [Checkpointer] Loading from dataset/check/voc/1726356812/base1/model_final-fsod.pth ...
WARNING [09/17 12:36:27 fvcore.common.checkpoint]: Some model parameters or buffers are not found in the checkpoint:
roi_heads.box_predictor.fc.weight
roi_heads.box_predictor.fsup
[09/17 12:36:27 d2.engine.train_loop]: Starting training from iteration 0
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 125, in _main
    prepare(preparation_data)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
ERROR [09/17 12:36:36 d2.engine.train_loop]: Exception during training:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 990, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/queues.py", line 107, in get
    if not self._poll(timeout):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/connection.py", line 424, in _poll
    r = wait([self], timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 42255) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 267, in run_step
    data = next(self._data_loader_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/data/common.py", line 234, in __iter__
    for d in self.dataset:
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1186, in _next_data
    idx, data = self._get_data()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1152, in _get_data
    success, data = self._try_get_data()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1003, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 42255) exited unexpectedly
[09/17 12:36:36 d2.engine.hooks]: Total training time: 0:00:08 (0:00:00 on hooks)
[09/17 12:36:36 d2.utils.events]:  iter: 0    lr: N/A  max_mem: 458M
Traceback (most recent call last):
  File "main.py", line 38, in <module>
    launch(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 67, in launch
    mp.spawn(
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 990, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/queues.py", line 107, in get
    if not self._poll(timeout):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/connection.py", line 257, in poll
    return self._poll(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/connection.py", line 424, in _poll
    r = wait([self], timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 42248) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/launch.py", line 126, in _distributed_worker
    main_func(*args)
  File "/users/acr23hk/paper/fsod-dc/main.py", line 30, in main
    return trainer.train()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 484, in train
    super().train(self.start_iter, self.max_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 149, in train
    self.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/defaults.py", line 494, in run_step
    self._trainer.run_step()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/engine/train_loop.py", line 267, in run_step
    data = next(self._data_loader_iter)
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/detectron2/data/common.py", line 234, in __iter__
    for d in self.dataset:
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1186, in _next_data
    idx, data = self._get_data()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1152, in _get_data
    success, data = self._try_get_data()
  File "/users/acr23hk/.conda/envs/paper1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1003, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 42248) exited unexpectedly

Traceback (most recent call last):
  File "main.py", line 7, in <module>
    import src.modeling
  File "/users/acr23hk/paper/fsod-dc/src/modeling/__init__.py", line 1, in <module>
    from . import rcnn, roi_heads
  File "/users/acr23hk/paper/fsod-dc/src/modeling/rcnn.py", line 10, in <module>
    from .refine import Refine
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 11, in <module>
    class Refine(nn.Module):
  File "/users/acr23hk/paper/fsod-dc/src/modeling/refine.py", line 128, in Refine
    alpha = self.momentum
NameError: name 'self' is not defined
